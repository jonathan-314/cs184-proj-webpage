<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
</style>
<title>CS 184 Pathtracer 1</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3: Pathtracer 1</h1>
<h2 align="middle">Jonathan Guo, Wentinn Liao, cs184jgwl</h2>

<br><br>

<div>

<h2 align="middle">Overview</h2>
<p>In this project, we implemented some path and ray tracing algorithms...</p>



<h2 align="middle">Part I: Ray Generation and Scene Intersection</h2>

<p>To generate rays from the camera, first we took the x and the y values and translated them into the camera space x and y by multiplying by the width of the viewplane and translating appropriately. Next, we turned them into world space by multiplying by the c2w matrix. Finally, there was the normalization and adding of position that occurs normally. </p>

<p>The triangle intersection algorithm we implemented was based off slide 22 in lecture 9, the optimized version. But what it basically does is that we know that the point of intersection of the ray and the triangle is on the plane of the triangle, which means the dot product between the plane's normal and a vector on the plane to the point of intersection is zero. Once there, we can compute the barycentric coordinates of the point of intersection with respect to the triangle, and it is inside the triangle if and only if all of the coordinates are non-negative. </p>

<div align="middle">
	<table>
		<tr>
			<td>
    <img src="images/Task 1/teapot.png" align="middle" width="400px"/>
    <figcaption align="middle"><em>dae/meshedit/teapot.dae</em>.</figcaption></td>
    <td>
    <img src="images/Task 1/cow.png" align="middle" width="400px"/>
    <figcaption align="middle"><em>dae/meshedit/cow.dae</em>.</figcaption></td>
</tr>
</table>
</div>


<h2 align="middle">Part II: Bounding Volume Hierarchy</h2>

<p>Our Bounding Volume Hierarchy construction algorithm took the longest side of the box and split the objects by the midpoint of that side. It terminated if the all of the primitives were on one side of the midpoint. Although this is not ideal, in practice it almost always does not occur until the very end when there are very few primitives left. </p>

<div align="middle">
	<img src="images/Task 2/cow.png" align="middle" width="400px"/>
    <figcaption align="middle"><em>dae/meshedit/cow.dae</em>.</figcaption></td>
</div>
<div align="middle">
	<img src="images/Task 2/maxplanck.png" align="middle" width="400px"/>
    <figcaption align="middle"><em>dae/meshedit/maxplanck.dae</em>.</figcaption></td>
</div>
<div align="middle">
	<img src="images/Task 2/CBlucy.png" align="middle" width="400px"/>
    <figcaption align="middle"><em>dae/sky/CBlucy.dae</em>.</figcaption></td>
</div>


<h2 align="middle">Part III: Bezier Surfaces with Separable 1D de Casteljau</h2>

<p>The direct lighting function with uniform hemispheric sampling treats all samples identically. For each sample, we sample a random vector in a <strong>z</strong>-positive hemisphere. Because this is <strong>z</strong>-positive with respect to the object, this is the incoming direction in the object space, <strong>w_in_o</strong> which is translated to the incoming direction in the world space <strong>w_in_w</strong> using <strong>o2w</strong>. We then use the BVH data structure to compute the first intersection of the uniformly sampled ray with an object, denoted <strong>incoming_isect</strong>, and obtain its emitted radiance. We then multiply by the BSDF at the camera ray intersection with the outgoing ray to the camera and the incoming ray from the emitting object and cosine of the incoming angle and add to a cumulative sum of radiance. Finally, we divide by the uniform PDF (or multiply by <strong>2 pi</strong>, and divide by the total number of samples.</p>

<div align="middle">
    <img src="images/Task 3/Uniform Hemispheric Sampling.png" align="middle" width="400px"/>
    <figcaption align="middle">Bunny Uniform Hemispheric Sampling at Rate 32.</figcaption>
</div>

<p>Importance lighting sampling iterates over each light for <strong>ns_area_light</strong> except for delta lights for which we only sample once. Calling <strong>sample_L</strong> from the light data structure gives the incoming direction in the world space, distance to the light, the PDF, and the radiance itself. The incoming ray is translated to object space and the ray is clipped to the time to the light so the BVH structure can be used to check for intersections with any other objects. If there is no intersection, then the same lighting equation is used, where the radiance is multiplied by the BSDF at the pixel intersection, the cosine of the incoming angle, and divided by the PDF and number of samples. The difference here is that we are keeping track of a sum where PDFs are non-uniform, and the number of samples across lights are different. We cannot make the same assumptions as before that allow us to optimize by doing multiplications at the end, so importance sampling does so properly during the loop.</p>

<div align="middle">
    <img src="images/Task 3/Importance Lighting Sampling.png" align="middle" width="400px"/>
    <figcaption align="middle">Bunny Importance Lighting at Rate 32.</figcaption>
</div>

<p>Lighting sampling is a form of importance sampling in which we sample with higher probability the rays that will contribute the most to the integral. Intuitively in this case, the rays that have been determined to "contribute the most" are ones that point to a light source. Particularly in this case, we are choosing to sample from a close-to-uniform distribution in the solid angle of rays to each light source, and zero distribution elsewhere. In effect, rather than integrating over the entire hemisphere, we are summing integrations over only solid angle regions that correspond to light sources. Because the number of samples remains the same, we are redirecting all samples to regions we know have a higher chance of reflecting light, which significantly improves the quality of the rendering. In general, importance sampling might not explicitly exclude particular regions of the integral, but in can be interpreted to be, with higher probability, allocating more samples to regions of greater or explicit interest to better estimate their integrals.</p>

<p>Shown below are images of a corner of the bunny in which soft shadows are sampled at a single ray per pixel, and at sampling rates of 1, 4, 16, and 64 respectively. We can see that the lower sampling rates are significantly more noisy. This is expected because the concept of soft shadow is centered on pixels at similar positions varying in shading not due to differences in distance or angles, but due to varying amounts of occlusion in lighting. With lower samples, the variation in occlusion is much less fine grained since averaging over fewer samples produces a higher variance estimate. For example, at a sampling rate of 1, all pixels within the shadow of the bunny now have shading that is binary, determined by whether a single random ray hits the bunny or not, which gives it the noisy appearance. As the number of samples increases, the average becomes a much more consistent approximation of the exact integral of occlusion per pixel.</p>

<div align="middle">
    <table style="width=100%">
        <tr>
            <td>
                <img src="images/Task 3/Soft Shadow Rate 1.png" align="middle" width="300px" style="margin: 20px"/>
                <figcaption align="middle">Soft shadow with sampling rate of 1.</figcaption>
            </td>
            <td>
                <img src="images/Task 3/Soft Shadow Rate 4.png" align="middle" width="300px" style="margin: 20px"/>
                <figcaption align="middle">Soft shadow with sampling rate of 4.</figcaption>
            </td>
        </tr>
        <br>
        <tr>
            <td>
                <img src="images/Task 3/Soft Shadow Rate 16.png" align="middle" width="300px" style="margin: 20px"/>
                <figcaption align="middle">Soft shadow with sampling rate of 16.</figcaption>
            </td>
            <td>
                <img src="images/Task 3/Soft Shadow Rate 64.png" align="middle" width="300px" style="margin: 20px"/>
                <figcaption align="middle">Soft shadow with sampling rate of 64.</figcaption>
            </td>
        </tr>
    </table>
</div>



<h2 align="middle">Section II: Triangle Meshes and Half-Edge Data Structure</h2>

<h3 align="middle">Part 3: Area-Weighted Vertex Normals</h3>

<p>To reduce repeated code, we noted that the existing <strong>normal()</strong> function of <strong>Face</strong> already computes a normal vector whose norm is twice the area of the face itself, however, returns the normalized version. Thus, we wrote a function <strong>areaWeightedNormal()</strong> that returns the penultimate result of the code for <strong>normal()</strong>, and modify <strong>normal()</strong> to call that.</p>

<p>Then, to iterate over the faces, we select a starting halfedge then iterate by changing <strong>h</strong> to <strong>h->twin()->next()</strong>. Each iteration, we retrieve the face the halfedge is on, and accumulate the sum of the area weighted normals. This produces a weighted sum of the face normals adjacent to the vertex, and at the end we simply call <strong>unit()</strong> to normalize.</p>

<div align="middle">
    <img src="images/Task 3 Teapot.png" align="middle" width="400px"/>
    <figcaption align="middle"><em>Area Weighted Vertex Normals: dae/teapot.dae</em>.</figcaption>
</div>


<h3 align="middle">Part 4: Edge Flip</h3>

<p>To implement Edge Flip, first we gathered all of the faces, vertices, and half edges necessary. Then, we set the neighbors of all 6 half edges, and then set the appropriate pointers of the faces and vertices. Note that we did not create a new half edge; we took the old one and just changed where it pointed from and to. </p>

<p>We ran into an error where we were unable to set the half edge of faces and vertices because it was a protected field, but by playing around with pointers and stuff we were able to get it to work. </p>
<div align="middle">
    <img src="images/Task 4 Teapot 0.png" align="middle" width="400px"/>
    <figcaption align="middle">Teapot before many edge flips</figcaption>
</div>
<p>The image above shows the teapot before edge flips. </p>
<div align="middle">
    <img src="images/Task 4 Teapot 1.png" align="middle" width="400px"/>
    <figcaption align="middle">Teapot after many edge flips</figcaption>
</div>
<p>The image above shows the teapot after some edge flips. We were able to carve a slab through the middle of the teapot. </p>
<div align="middle">
    <img src="images/Task 4 Teapot 2.png" align="middle" width="400px"/>
    <figcaption align="middle">Teapot after many edge flips</figcaption>
</div>
<p>The image above shows the same teapot as before, but using the smooth shading instead. </p>


<h3 align="middle">Part 5: Edge Split</h3>

<p>Carefully. The overall implementation was separated into 5 sections: <ul>
        <li>Getting all vertex, halfedge, and face objects and naming in a way that is easy to understand</li>
        <li>Creating the midpoint, new halfedges, edges, and faces with the same naming convention as before</li>
        <li>Setting pointers for new objects</li>
        <li>Setting pointers for persistent old objects</li>
        <li>Deleting nonpersistent old objects</li>
    </ul> Small things that were almost missed were making sure to update the four original halfedges with new faces, and making sure to correctly delete the original faces. Keeping track of all data structures and variables that needed to be changed allowed us to implement it correctly first try.</p>

<div align="middle">
    <img src="images/Task 5/Edge Split.png" align="middle" width="400px"/>
    <figcaption align="middle">Edge splits <em>dae/teapot.dae</em>.</figcaption>
</div>
<div align="middle">
    <img src="images/Task 5/Edge Split and Flip.png" align="middle" width="400px"/>
    <figcaption align="middle">Combination of edge splits and flips <em>dae/teapot.dae</em>.</figcaption>
</div>


<h3 align="middle">Part 6: Loop Subdivision for Mesh Upsampling</h3>

<p>We implemented loop subdivision using the recommended method. First we calculated the new position of both the new vertices as well as the old vertices. Next, we split every edge. Here, because the list of edges is stored as a linked list, and new edges are appended to the end, what we did was we knew we only had to split a certain number of edges, so we blindly used a for loop to split that number of edges. This way we could use the isNew flag for the flipping. After splitting we did flipping and then set the vertex positions to the calculated ones. </p>

<p>Sharp edges and corners become less sharp (more rounded) after loop subdivision. This is because during loop subdivision, we move each vertex to a new position based on its neighbors. So, if a vertex or edge is very sharp, it will be moved closer to its neighbors and become less sharp. </p>

<p>To reduce this effect, we can split some edges that are near the sharp edge/corner. This way, when the vertices are moved during the loop subdivision, the sharp edge/corner won't be moved as much, thereby reducing the effect. </p>

<p>We can preprocess the cube to avoid assymetry. The way to do this is to split every edge that goes along the face of the cube. This way, each face has an X instead of just a \ or a /, leading to more symmetry. </p>
<div align="middle">
	<table style="with=100%">
		<tr>
            <td>
                <img src="images/Task 6/cube 0.png" align="middle" width="400px"/>
                <figcaption align="middle">Cube after preprocessing<em>./dae/cube.dae</em>.</figcaption>
            </td>
            <td>
                <img src="images/Task 6/cube 1.png" align="middle" width="400px"/>
                <figcaption align="middle">Cube after preprocessing<em>./dae/cube.dae</em>.</figcaption>
            </td>
        </tr>
    </table>
</div>
<p>Next, once we upsample once, we see that the cube is symmetrical, because each face (before the upsampling) was symmetrical. </p>
<div align="middle">
	<table style="with=100%">
		<tr>
            <td>
                <img src="images/Task 6/cube 2.png" align="middle" width="400px"/>
                <figcaption align="middle">Cube after 1 loop subdivision<em>./dae/cube.dae</em>.</figcaption>
            </td>
            <td>
                <img src="images/Task 6/cube 3.png" align="middle" width="400px"/>
                <figcaption align="middle">Cube after 1 loop subdivision<em>./dae/cube.dae</em>.</figcaption>
            </td>
        </tr>
    </table>
</div>
<p>We can also do more loop subdivisions. </p>
<div align="middle">
	<table style="with=100%">
		<tr>
            <td>
                <img src="images/Task 6/cube 4.png" align="middle" width="400px"/>
                <figcaption align="middle">Cube after 2 loop subdivisions<em>./dae/cube.dae</em>.</figcaption>
            </td>
            <td>
                <img src="images/Task 6/cube 5.png" align="middle" width="400px"/>
                <figcaption align="middle">Cube after 3 loop subdivisions<em>./dae/cube.dae</em>.</figcaption>
            </td>
        </tr>
    </table>
</div>


</body>
</html>
